{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mustafabozkaya/DeepLearningBootcamp2022/blob/master/classsification_urbansounds_with_cnn_kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[<img align=\"Left\" width=\"100\" height=\"100\" src=\"https://thumbs.dreamstime.com/b/mb-initial-letter-vector-logo-icon-mb-initial-letter-vector-logo-icon-204517753.jpg\">](https://github.com/mustafabozkaya)"
      ],
      "metadata": {
        "id": "0tQXZ7fatOXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spectogram Recognition with CNN\n",
        "\n",
        "---\n",
        "[<img align=\"Left\" width=\"800\" height=\"300\" src=\"https://www.researchgate.net/publication/319081627/figure/fig1/AS:534034566004736@1504335170521/Spectrogram-of-a-speech-signal-with-breath-sound-marked-as-Breath-whose-bounds-are.png\">](#)\n",
        "\n"
      ],
      "metadata": {
        "id": "FIOQmr47UV4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n"
      ],
      "metadata": {
        "id": "RSRCmdYyvd8n",
        "outputId": "0832e48b-d4e8-4e7b-e4fa-a215afa0bdbf",
        "execution": {
          "iopub.status.busy": "2022-10-12T20:59:37.797101Z",
          "iopub.execute_input": "2022-10-12T20:59:37.797856Z",
          "iopub.status.idle": "2022-10-12T20:59:38.917848Z",
          "shell.execute_reply.started": "2022-10-12T20:59:37.797745Z",
          "shell.execute_reply": "2022-10-12T20:59:38.916370Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/working\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and Extracting the Dataset"
      ],
      "metadata": {
        "id": "ZZg9UvyOJ4J7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz"
      ],
      "metadata": {
        "id": "j8303pDbJ4J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "8_wxcKC60W7E",
        "outputId": "2504be96-d6f5-441a-80ff-f22fcb61c6d1",
        "execution": {
          "iopub.status.busy": "2022-10-12T21:00:57.261650Z",
          "iopub.execute_input": "2022-10-12T21:00:57.262130Z",
          "iopub.status.idle": "2022-10-12T21:00:58.356150Z",
          "shell.execute_reply.started": "2022-10-12T21:00:57.262086Z",
          "shell.execute_reply": "2022-10-12T21:00:58.354941Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "__notebook_source__.ipynb\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf UrbanSound8K.tar.gz #extract tar file"
      ],
      "metadata": {
        "id": "FAbJLMxxJ4J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la UrbanSound8K/"
      ],
      "metadata": {
        "id": "VSoiJ6kD1hhd",
        "outputId": "fe0c5f1f-c312-451f-cb87-b34eaf364e9b",
        "execution": {
          "iopub.status.busy": "2022-10-12T21:01:39.563195Z",
          "iopub.execute_input": "2022-10-12T21:01:39.564626Z",
          "iopub.status.idle": "2022-10-12T21:01:40.650046Z",
          "shell.execute_reply.started": "2022-10-12T21:01:39.564572Z",
          "shell.execute_reply": "2022-10-12T21:01:40.648726Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "ls: cannot access 'UrbanSound8K/': No such file or directory\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat UrbanSound8K/UrbanSound8K_README.txt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-12T21:01:55.396524Z",
          "iopub.execute_input": "2022-10-12T21:01:55.396954Z",
          "iopub.status.idle": "2022-10-12T21:01:56.487662Z",
          "shell.execute_reply.started": "2022-10-12T21:01:55.396915Z",
          "shell.execute_reply": "2022-10-12T21:01:56.485802Z"
        },
        "trusted": true,
        "id": "sLZaQqxkJ4J-",
        "outputId": "a9bf931d-1b1c-42aa-dc9b-b9534b1390c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "cat: UrbanSound8K/UrbanSound8K_README.txt: No such file or directory\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for root ,dir ,file in os.walk(\"../working/UrbanSound8K/\"):\n",
        "    print(root)\n",
        "    print(dir)\n",
        "    print(len(file))"
      ],
      "metadata": {
        "id": "hpQ9oRJ_J4J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Spectrograms"
      ],
      "metadata": {
        "id": "i_r-fIBGJ4KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"UrbanSound8K/metadata/UrbanSound8K.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "miIx2z3eJ4KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "3_IaUjoGJ4KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "-7mobnzqJ4KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the metadata file and read it into a pandas dataframe\n",
        "def create_spectrogram(y): # y is the audio time series\n",
        "    spec = librosa.feature.melspectrogram(y=y) # melspectrogram is a visual representation of the short-term power spectrum of a sound,retrieved from the audio time series\n",
        "    specdb = librosa.power_to_db(spec, ref=np.max) # power_to_db is used to convert a spectrogram from power (amplitude squared) units to decibel (dB) units , ref is used to set the reference power\n",
        "    spec_conv = librosa.amplitude_to_db(spec, ref=np.max) # amplitude_to_db is used to convert a spectrogram from amplitude units to decibel (dB) units , ref is used to set the reference power\n",
        "    \n",
        "    return spec_conv\n",
        "\n",
        "\n",
        "def save_spectrogram(spect_paths,spectrogram, file_name, classid):\n",
        "    if str(classid) not in os.listdir(spect_paths):\n",
        "        os.mkdir(f\"{spect_paths}/{classid}\")\n",
        "\n",
        "    save_name = file_name.split(\".\")[0]\n",
        "    \n",
        "    plt.figure()\n",
        "    librosa.display.specshow(spectrogram) # specshow is used to display a spectrogram\n",
        "    plt.savefig(f\"{spect_paths}/{classid}/{save_name}.png\", bbox_inches=\"tight\", pad_inches=0) # savefig is used to save the current figure,bbox_inches is used to trim the figure to the given bounding box in inches, pad_inches is used to specify the extra padding around the figure when bbox_inches is used\n",
        "    plt.close()\n",
        "# listen to a random sound from the dataset\n",
        "def listen_to_random_sound(data,sr):\n",
        "  \n",
        "    IPython.display.display(IPython.display.Audio(data, rate=sr)) # display is used to display the audio file"
      ],
      "metadata": {
        "id": "MuWrmIcZJ4KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd .."
      ],
      "metadata": {
        "id": "f6c5s2uNJ4KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "Ghm7lsyAJ4KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_folds = \"working/UrbanSound8K/audio\"\n",
        "output_path=\"./working\"\n",
        "\n",
        "if \"spectrograms\" not in os.listdir(output_path):\n",
        "\n",
        "    os.mkdir(f\"{output_path}/spectrograms\")\n",
        "number_of_files = df.shape[0] # get the rows\n",
        "number_of_processed = 0\n",
        "number_of_errors = 0\n",
        "\n",
        "\n",
        "with open(f\"{output_path}/errors.txt\", \"w\") as error_file:\n",
        "    for index, row in df.iterrows(): # iterrows is used to iterate over rows of a dataframe,retrieving the index and the row\n",
        "        try:\n",
        "            file_name = row[\"slice_file_name\"]\n",
        "            fold = row[\"fold\"]\n",
        "            classid = row[\"classID\"]\n",
        "            path_to_file = f\"{path_to_folds}/fold{fold}/{file_name}\"\n",
        "                        \n",
        "            data, sr = librosa.load(path_to_file)# load is used to load audio files with librosa,retrieving the time series and the sampling rate\n",
        "            # type(data) is numpy.ndarray\n",
        "            # type(sr) is int\n",
        "            #shape of data is (n,) where n is the number of samples\n",
        "            # sr is the sampling rate of the audio file\n",
        "            #print(f\" \\r  data type is {type(data)} and sampling rate is {type(sr)} - shape of data is {data.shape} and sampling rate is {sr}\",end=\"\")\n",
        "            #listen_to_random_sound(data,sr)\n",
        "            \n",
        "            spectrogram = create_spectrogram(data)\n",
        "            spects_path=f\"{output_path}/spectrograms\"\n",
        "            save_spectrogram(spects_path,spectrogram, file_name, classid)\n",
        "            \n",
        "            del data\n",
        "            del sr\n",
        "            del spectrogram\n",
        "            \n",
        "        except Exception as e:\n",
        "            number_of_errors += 1\n",
        "            error_file.write(f\"{number_of_errors}: {e}\\n\")\n",
        "        \n",
        "        finally:\n",
        "            number_of_processed += 1\n",
        "        \n",
        "        print(f\"\\rNumber: {number_of_processed}/{number_of_files} | Errors: {number_of_errors}\", end=\"\")\n",
        "        \n",
        "        #break"
      ],
      "metadata": {
        "id": "BKuEqEnUJ4KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and  Save İmage datasets "
      ],
      "metadata": {
        "id": "1s7GwkpJORUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create numpy arrays for the training images and labels\n",
        "def create_training_data(datasetfolder_path,resize=None,grayscale=False,normalize=False):\n",
        "\n",
        "    training_data = []\n",
        "    number_of_errors = 0\n",
        "    number_of_processed = 0\n",
        "\n",
        "    for classid in os.listdir(datasetfolder_path):\n",
        "        path_to_class = f\"{datasetfolder_path}/{classid}\"\n",
        "        \n",
        "        for img in os.listdir(path_to_class):\n",
        "            try:\n",
        "                if grayscale:\n",
        "                    img_array = cv.imread(os.path.join(\n",
        "                        path_to_class, img), cv.IMREAD_GRAYSCALE)\n",
        "                # imread is used to read an image from the specified file,IMREAD_GRAYSCALE is used to load an image in grayscale mode,retrieving the image as a numpy array\n",
        "\n",
        "                else:\n",
        "                    img_array = cv.imread(f\"{path_to_class}/{img}\")\n",
        "\n",
        "                if resize is not None:\n",
        "                    img_array = cv.resize(img_array, resize)\n",
        "                    # resize is used to resize an image,retrieving the resized image as a numpy array\n",
        "                \n",
        "                if normalize:\n",
        "                    img_array = img_array/255.0\n",
        "                    # normalize the image array\n",
        "\n",
        "                training_data=np.append([img_array, classid])\n",
        "                number_of_processed += 1\n",
        "\n",
        "            except:\n",
        "                number_of_errors += 1\n",
        "\n",
        "        print(f\"\\rProcessed: {number_of_processed} | Erros: {number_of_errors} | training data shape : {np.ndarray(training_data).shape} \", end=\"\")\n",
        "    return training_data"
      ],
      "metadata": {
        "id": "EewCdQgSJ4KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_datasets=create_training_data(datasetfolder_path=f\"{output_path}/spectrograms\")\n",
        "len(img_datasets)"
      ],
      "metadata": {
        "id": "sDobCjUMJ4KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image ,id in images:\n",
        "  print(f\"\\r class id :{id}  image :{image.shape}\", end=\"\")"
      ],
      "metadata": {
        "id": "brTVJimyTL7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the csv file for the training data\n",
        "def write_csv(training_data, csv_name):\n",
        "    df = pd.DataFrame(training_data, columns=[\"image\", \"classid\"])\n",
        "    df.to_csv(csv_name, index=False)\n",
        "    print(\"CSV file created\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "W0h1v6eDJ4KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pickle(path,data):\n",
        "    if \"pickle_img_dataset\" not in os.listdir(path):\n",
        "    os.mkdir(f\"{path}/pickle_img_dataset\")\n",
        "\n",
        "    with open((f\"{path}/pickle_img_dataset/allimg8732.pickle\", \"wb\") as f:\n",
        "      pickle.dump(data, f)"
      ],
      "metadata": {
        "id": "rTZ-1_rLJ4KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "NtFJlQafLLb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir=f\"{output_path}/spectrograms/\"\n"
      ],
      "metadata": {
        "id": "LMzna52zK_ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = pathlib.Path(dataset_dir)\n",
        "image_count = len(list(data_dir.glob('*/*.png')))\n",
        "print(image_count)"
      ],
      "metadata": {
        "id": "Nga_Hoob94AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spec = list(data_dir.glob('1/*'))\n",
        "PIL.Image.open(str(spec[6]))"
      ],
      "metadata": {
        "id": "qMVc71-y-pcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for root, dirs, files in os.walk(dataset_dir, topdown=True):\n",
        "    print(\"=\"*50)\n",
        "    print(root)\n",
        "    print(f\"root type :{type(files)}\")\n",
        "    print(f\"root lenth :{len(files)}\")\n",
        "    print(files)\n",
        "    print(f\"files type :{type(files)}\")\n",
        "    print(f\"files lenth :{len(files)}\")\n",
        "    print(dirs)\n",
        "    print(f\"DİR type :{type(dirs)}\")\n",
        "    print(f\"DİR lenth :{len(dirs)}\")\n",
        "    print(\"=\"*50)\n",
        "    break\n",
        "    "
      ],
      "metadata": {
        "id": "MqBw9qi-xAHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for root,dirs, files in os.walk(dataset_dir):\n",
        "    for file in (files):\n",
        "        print(os.path.join(root, file))\n",
        "        break\n",
        "    "
      ],
      "metadata": {
        "id": "P9lYJtNq3GbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ],
      "metadata": {
        "id": "YK9jk6b8f5pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_name=\"1\" # set to class name\n",
        "\n",
        "train_files = glob.glob(str(os.path.join(dataset_dir,str(class_name)))+'/*')\n",
        "\n",
        "\n",
        "print(*train_files)\n",
        "print(f\"1 spectogram img lenth :{len(train_files)}\")\n",
        "train_imgs = [img_to_array(load_img(img)) for img in train_files]\n",
        "train_imgs = np.array(train_imgs)\n",
        "train_labels = np.array([fn.split('/')[5].split(\".\")[0].strip() for fn in train_files]) # target , y label\n",
        "\n",
        "print('Train dataset shape:', train_imgs.shape)\n",
        "print('Train labels shape:', train_labels.shape)"
      ],
      "metadata": {
        "id": "vW2Y0qRp1Jny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create image datasets "
      ],
      "metadata": {
        "id": "oxvvs7VZ1BCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size=(374,500)\n",
        "batch_size=32 # paketler\n",
        "\n",
        "#Setting train/test split\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \n",
        "    directory=dataset_dir,\n",
        "    labels=\"inferred\",\n",
        "    validation_split=0.3,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=dataset_dir,\n",
        "    labels=\"inferred\",\n",
        "    validation_split=0.3,\n",
        "    subset=\"validation\",\n",
        "    seed=1007,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "WEdnyOxsK-GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{train_ds.class_names}\")"
      ],
      "metadata": {
        "id": "BqAPH1xC_Cge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print how many images are in the training and validation datasets\n",
        "print(\"Number of training images: \", len(train_ds))\n",
        "print(\"Number of validation images: \", len(valid_ds))"
      ],
      "metadata": {
        "id": "sW1f6TE9N0bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split valid dataset to test and valid\n",
        "test_ds = valid_ds.take(30) # get 100 data from valid dataset\n",
        "valid_ds = valid_ds.skip(30) # skip first 100"
      ],
      "metadata": {
        "id": "VR1uhAoTN5wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how many bachs are in the training and validation datasets\n",
        "print(\"Number of training batches: \", len(train_ds))\n",
        "print(\"Number of validation batches: \", len(valid_ds))\n",
        "print(\"Number of test  batches: \", len(test_ds))"
      ],
      "metadata": {
        "id": "gx_5Hyu2N701"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_type=[train_ds,valid_ds,test_ds]"
      ],
      "metadata": {
        "id": "JaS0ArE1TsD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ds in dataset_type:\n",
        "  print(type(ds))"
      ],
      "metadata": {
        "id": "9gowy3cs-IvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking images and labels shapes (amount of images, height, width, color channels)\n",
        "for image_batch, labels_batch in test_ds.take(2):\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "xw2kIT18A0zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking images and labels shapes (amount of images, height, width, color channels)\n",
        "for image_batch, labels_batch in valid_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "GncMbIISq3Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualizaiton"
      ],
      "metadata": {
        "id": "CcCy6z4c-kqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying image samples \n",
        "plt.figure(figsize=(18, 18))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(batch_size):\n",
        "        ax = plt.subplot(4,8,i+1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "U3kPRAYpB1kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying image samples \n",
        "plt.figure(figsize=(18, 18))\n",
        "for images, labels in valid_ds.take(1):\n",
        "    for i in range(batch_size):\n",
        "        ax = plt.subplot(4,8,i+1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "XeIKzAz8q9Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying image samples \n",
        "plt.figure(figsize=(18, 18))\n",
        "for images, labels in test_ds.take(1):\n",
        "    for i in range(batch_size):\n",
        "        ax = plt.subplot(4,8,i+1)\n",
        "        #print(f\"type image {images[i]}\")\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "JaRjjyD2q9Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "hcQc1f2BiLot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can build our model and start training, we need to apply one simple augmentation the dataset and that is rescaling. We rescale an input in the (0, 255) range to be in the (0,1) range."
      ],
      "metadata": {
        "id": "6QJ0dI2piLou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# convert rgb img to gray scale img\n",
        "def convert_to_gray_scale(img):\n",
        "   \n",
        "    img = tf.image.rgb_to_grayscale(img)\n",
        "   \n",
        "    print(f\"\\r convert gray :  image shape : {img.shape} | img type: {type(img)} \", end=\"\")\n",
        "    return img\n",
        "\n"
      ],
      "metadata": {
        "id": "fS8aFSZ0y47t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dj_Tu43p_64T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to prepare our datasets for modelling\n",
        "def prepare(ds, augment=False, batch_size=None, resized_size=None,grayscale=True):\n",
        "    # Define normalisation function\n",
        "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "    # gray_scale transfpormayion \n",
        "    resize=tf.keras.Sequential(\n",
        "        [tf.keras.layers.experimental.preprocessing.Resizing(\n",
        "            resized_size[0],\n",
        "            resized_size[1],\n",
        "            crop_to_aspect_ratio=True,\n",
        "        )]\n",
        "    )\n",
        "     #define data augmentation function\n",
        "    flip_and_rotate = tf.keras.Sequential([\n",
        "        tf.keras.layers.experimental.preprocessing.RandomFlip(\n",
        "            \"horizontal_and_vertical\"),\n",
        "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
        "    ])\n",
        "\n",
        "    if grayscale:\n",
        "        ds = ds.map(lambda x, y: (convert_to_gray_scale(x), y))\n",
        "    if resized_size:\n",
        "        ds = ds.map(lambda x, y: (resize(x, training=True), y))\n",
        "\n",
        "  \n",
        "    ds = ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "    image_batch, labels_batch = next(iter(ds))\n",
        "    print(f\"batch size :{image_batch.shape}\")\n",
        "    print(f\"label size :{labels_batch.shape}\")\n",
        "    first_image = image_batch[0]\n",
        "    # Notice the pixel values are now in `[0,1]`.\n",
        "    print(np.min(first_image), np.max(first_image))\n",
        "\n",
        "    if augment:\n",
        "        ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n",
        "\n",
        "        \n",
        "    \n",
        "    \n",
        "    return ds\n",
        "\n",
        "image_size= (187,250)\n",
        "train_dataset = prepare(train_ds, augment=False, batch_size=batch_size, resized_size=image_size)\n",
        "valid_dataset = prepare(test_ds, augment=False, batch_size=batch_size, resized_size=image_size)\n",
        "test_dataset = prepare(valid_ds, augment=False, batch_size=batch_size, resized_size=image_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "zUTJeOtZiLou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_batch, labels_batch in valid_dataset:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "fymkVSDq3Med"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying image samples \n",
        "plt.figure(figsize=(18, 12))\n",
        "for images, labels in train_dataset.take(1):\n",
        "    for i in range(8):\n",
        "        ax = plt.subplot(2,4,i+1)\n",
        "        #print(f\"type image {images[i]}\")\n",
        "        plt.imshow(images[i].numpy().squeeze(axis=2),cmap=\"gray\")\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "6n2_EUpq0vvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling"
      ],
      "metadata": {
        "id": "iQKd5RauPB82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if the data format i.e the RGB channel is coming first or last so, whatever it may be, model will check first and then input shape will be feeded accordingly.\n",
        "from keras import backend as K\n",
        "image_size= (187,250)\n",
        "img_height=image_size[0]\n",
        "img_width=image_size[1]\n",
        "if K.image_data_format() == \"channels_first\":\n",
        "    input_shape = (1, img_height, img_width)\n",
        "else:\n",
        "    input_shape = (img_height, img_width, 1)\n"
      ],
      "metadata": {
        "id": "o5EdaBRDPGd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "1GzWjmyCKF5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_net(select_model=1): \n",
        "  model = tf.keras.Sequential()\n",
        "  if select_model==1:\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(96, kernel_size = (3,3), input_shape = input_shape, strides = (2,2), activation = 'relu'))\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size = (3,3),strides = (2,2))) \n",
        "    model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1) ,activation='relu', padding=\"same\")) \n",
        "    model.add(tf.keras.layers.BatchNormalization()) \n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2))) \n",
        "    model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")) \n",
        "    model.add(tf.keras.layers.BatchNormalization()) \n",
        "    model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")) \n",
        "    model.add(tf.keras.layers.BatchNormalization()) \n",
        "    model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")) \n",
        "    model.add(tf.keras.layers.BatchNormalization()) \n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2))) \n",
        "\n",
        "    model.add(tf.keras.layers.Flatten()) \n",
        "\n",
        "    model.add(tf.keras.layers.Dense(4096, activation='relu')) \n",
        "    model.add(tf.keras.layers.Dropout(0.5)) \n",
        "    model.add(tf.keras.layers.Dense(4096, activation='relu')) \n",
        "    model.add(tf.keras.layers.Dropout(0.5)) \n",
        "    model.add(tf.keras.layers.Dense(2048, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax')) \n",
        "\n",
        "  elif select_model==2:\n",
        "    # Create CNN model\n",
        "  \n",
        "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(\n",
        "        32, 3, strides=2, padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "  elif select_model==3:\n",
        "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
        "    model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "  else:\n",
        "\n",
        "    model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(input_shape)))\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.MaxPooling2D())\n",
        "    model.add(tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.MaxPooling2D())\n",
        "    model.add(tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.MaxPooling2D())\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "  \n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "    \n",
        "  return model"
      ],
      "metadata": {
        "id": "4sDxeRqwwDxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dl=conv_net(select_model=3)"
      ],
      "metadata": {
        "id": "rLQJ4XEJLkj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Visualization"
      ],
      "metadata": {
        "id": "dkZBKOuciLov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dl.summary()\n"
      ],
      "metadata": {
        "id": "7I3CppJTWrSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Compile model\n",
        "model_dl.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.RMSprop(),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Train model for 10 epochs, capture the history\n",
        "history = model_dl.fit(train_dataset,\n",
        "          batch_size=batch_size,\n",
        "          verbose=1,\n",
        "          epochs=50,\n",
        "          validation_data=valid_dataset)"
      ],
      "metadata": {
        "id": "b-vFRrUpRsEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "F1g3cwBwXv13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_dl.fit(x_train, y_train,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           verbose=1,\n",
        "#           validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "KQ2cRjQsXucS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# score = model_dl.evaluate(x_test, y_test, verbose=0)\n",
        "# score = model_dl.evaluate(test_dataset, verbose=0)\n",
        "# print(f\"Loss: {score[0]} - Accuracy: {score[1]}\")\n"
      ],
      "metadata": {
        "id": "JGPF05mLakxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Saving"
      ],
      "metadata": {
        "id": "zs1dmePNbNV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_test = model_dl.save('save_models/spectogram_model.h5')"
      ],
      "metadata": {
        "id": "G9DrBMvzcvIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "h3qUBCIbgoWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compute the final loss and accuracy score on our valid dataset using the evaluate() function."
      ],
      "metadata": {
        "id": "zroE6ow-iLow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the final loss and accuracy\n",
        "final_loss, final_acc = model_dl.evaluate(valid_dataset, verbose=0)\n",
        "print(\"Final loss: {0:.6f}, final accuracy: {1:.6f}\".format(\n",
        "    final_loss, final_acc))\n"
      ],
      "metadata": {
        "id": "Afhq5RAQiLow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves for training and validation.\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, len(loss_values)+1)\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "plt.plot(epochs, loss_values, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "60chUuK_gq3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy curves for training and validation.\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, len(acc_values)+1)\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "plt.plot(epochs, acc_values, 'y', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mQbvQiiJiLow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(test_dataset))[1].numpy()[8]"
      ],
      "metadata": {
        "id": "L6lSxyQrrqJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testimg=next(iter(test_dataset))[0].numpy()[8]"
      ],
      "metadata": {
        "id": "QRgduNwFsZ1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testimg.shape"
      ],
      "metadata": {
        "id": "BUpcfeUms7tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testimg=np.expand_dims(testimg,axis=0)"
      ],
      "metadata": {
        "id": "GHVsal8utDtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred=model_dl.predict(testimg)"
      ],
      "metadata": {
        "id": "bwzeb-bMiLow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "id": "obQn93-qsmGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(pred)"
      ],
      "metadata": {
        "id": "EQ7Z6elitTEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dKrnFPCTtbm9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}