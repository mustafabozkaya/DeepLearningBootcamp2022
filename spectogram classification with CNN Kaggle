{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mustafabozka/classsification-urbansounds-with-cnn-kaggle?scriptVersionId=107898794\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"[<img align=\"Left\" width=\"100\" height=\"100\" src=\"https://thumbs.dreamstime.com/b/mb-initial-letter-vector-logo-icon-mb-initial-letter-vector-logo-icon-204517753.jpg\">](https://github.com/mustafabozkaya)","metadata":{"id":"0tQXZ7fatOXo"}},{"cell_type":"markdown","source":"# Spectogram Recognition with CNN\n\n---\n[<img align=\"Left\" width=\"800\" height=\"300\" src=\"https://www.researchgate.net/publication/319081627/figure/fig1/AS:534034566004736@1504335170521/Spectrogram-of-a-speech-signal-with-breath-sound-marked-as-Breath-whose-bounds-are.png\">](#)\n\n","metadata":{"id":"FIOQmr47UV4n"}},{"cell_type":"code","source":"!pwd\n","metadata":{"id":"RSRCmdYyvd8n","outputId":"0832e48b-d4e8-4e7b-e4fa-a215afa0bdbf","execution":{"iopub.status.busy":"2022-10-12T20:59:37.797101Z","iopub.execute_input":"2022-10-12T20:59:37.797856Z","iopub.status.idle":"2022-10-12T20:59:38.917848Z","shell.execute_reply.started":"2022-10-12T20:59:37.797745Z","shell.execute_reply":"2022-10-12T20:59:38.91637Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Downloading and Extracting the Dataset","metadata":{}},{"cell_type":"code","source":"!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"id":"8_wxcKC60W7E","outputId":"2504be96-d6f5-441a-80ff-f22fcb61c6d1","execution":{"iopub.status.busy":"2022-10-12T21:00:57.26165Z","iopub.execute_input":"2022-10-12T21:00:57.26213Z","iopub.status.idle":"2022-10-12T21:00:58.35615Z","shell.execute_reply.started":"2022-10-12T21:00:57.262086Z","shell.execute_reply":"2022-10-12T21:00:58.354941Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"__notebook_source__.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"!tar -xzf UrbanSound8K.tar.gz #extract tar file","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -la UrbanSound8K/","metadata":{"id":"VSoiJ6kD1hhd","outputId":"fe0c5f1f-c312-451f-cb87-b34eaf364e9b","execution":{"iopub.status.busy":"2022-10-12T21:01:39.563195Z","iopub.execute_input":"2022-10-12T21:01:39.564626Z","iopub.status.idle":"2022-10-12T21:01:40.650046Z","shell.execute_reply.started":"2022-10-12T21:01:39.564572Z","shell.execute_reply":"2022-10-12T21:01:40.648726Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"ls: cannot access 'UrbanSound8K/': No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"!cat UrbanSound8K/UrbanSound8K_README.txt","metadata":{"execution":{"iopub.status.busy":"2022-10-12T21:01:55.396524Z","iopub.execute_input":"2022-10-12T21:01:55.396954Z","iopub.status.idle":"2022-10-12T21:01:56.487662Z","shell.execute_reply.started":"2022-10-12T21:01:55.396915Z","shell.execute_reply":"2022-10-12T21:01:56.485802Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"cat: UrbanSound8K/UrbanSound8K_README.txt: No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfor root ,dir ,file in os.walk(\"../working/UrbanSound8K/\"):\n    print(root)\n    print(dir)\n    print(len(file))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Copying current content to new editable directory\n#!cp -r \"../content/drive/MyDrive/spectrograms/\" \"/sample_data/\"\n","metadata":{"id":"lx1-IbDAAHkP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+\n# Package İmport and Controlling","metadata":{"id":"mmwhitluKIHx"}},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","metadata":{"id":"3I0a97d4KHgS","outputId":"d84fd1a9-2375-41fa-dc0e-1d007e372423"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!cat /proc/meminfo","metadata":{"id":"MorG7bVkEC-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q Pillow","metadata":{"id":"upGiOWYHLf9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ../content","metadata":{"id":"THmIw3mvK5VM","outputId":"ec205187-9f32-4c9e-bd99-4fdf50b4eac4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\n\n\nimport Pillow # it is a fork of the Python Imaging Library (PIL), which adds support for opening, manipulating, and saving many different image file formats.\nimport PIL.Image\n\nimport tensorflow_datasets as tfds\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n\nimport numpy as np\n\nimport glob\nimport shutil\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nimport random\nimport pickle # to save the model to disk, we use pickle to serialize the object to a byte stream and save it to disk\nimport librosa # for audio processing and feature extraction , librosa is a python package for music and audio analysis, it provides the building blocks necessary to create music information retrieval systems, it is built on top of the scientific python stack and numpy  and it is the most widely used python package for music and audio analysis and it is used for music information retrieval systems and it is built on top of the scientific python stack and numpy and it is the most widely used python package for music and audio analysis\n\nimport librosa.display # librosa.display is a module for visualization of audio data in python,it is built on top of matplotlib and numpy and it is the most widely used python package for music and audio analysis\nimport cv2 as cv # for image processing and feature extraction , cv2 mostly used for image processing and feature extraction\nimport tensorflow as tf # for deep learning and neural networks , tensorflow is an open source machine learning library for research and production, it is a symbolic math library and also used for machine learning applications such as neural networks\nfrom tf.keras.models import Sequential # sequential is a linear stack of layers\nfrom tf.keras.layers import Dense, Dropout, Activation, Flatten # dense is a regular deeply connected neural network layer, dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data, activation is used to activate the neurons, flatten is used to flatten the input\nfrom tf.keras.layers import Conv2D, MaxPooling2D # conv2d is a 2d convolution layer, maxpooling2d is a 2d max pooling layer \n\n\nplt.rcParams[\"figure.figsize\"]=(18,8)\n%matplotlib inline\nnp.random.seed(42)","metadata":{"id":"V45NzAqmLoet"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntf.test.gpu_device_name()\n","metadata":{"id":"qc_GcnEbCIbu","outputId":"5aa03694-2249-4741-8f5e-7f1fa5c91ee4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.__version__","metadata":{"id":"zd6kHvnfCgmS","outputId":"cf4e93d5-3bcd-4378-e699-602bb19fd306"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv.__version__","metadata":{"id":"0CECC3ML1u5G","outputId":"bb69093a-0705-4ebe-e7c4-63f17fda0c52"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.__version__","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.__version__","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.__version__","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Spectrograms","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"UrbanSound8K/metadata/UrbanSound8K.csv\")\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the metadata file and read it into a pandas dataframe\ndef create_spectrogram(y): # y is the audio time series\n    spec = librosa.feature.melspectrogram(y=y) # melspectrogram is a visual representation of the short-term power spectrum of a sound,retrieved from the audio time series\n    specdb = librosa.power_to_db(spec, ref=np.max) # power_to_db is used to convert a spectrogram from power (amplitude squared) units to decibel (dB) units , ref is used to set the reference power\n    spec_conv = librosa.amplitude_to_db(spec, ref=np.max) # amplitude_to_db is used to convert a spectrogram from amplitude units to decibel (dB) units , ref is used to set the reference power\n    \n    return spec_conv\n\n\ndef save_spectrogram(spect_paths,spectrogram, file_name, classid):\n    if str(classid) not in os.listdir(spect_paths):\n        os.mkdir(f\"{spect_paths}/{classid}\")\n\n    save_name = file_name.split(\".\")[0]\n    \n    plt.figure()\n    librosa.display.specshow(spectrogram) # specshow is used to display a spectrogram\n    plt.savefig(f\"{spect_paths}/{classid}/{save_name}.png\", bbox_inches=\"tight\", pad_inches=0) # savefig is used to save the current figure,bbox_inches is used to trim the figure to the given bounding box in inches, pad_inches is used to specify the extra padding around the figure when bbox_inches is used\n    plt.close()\n# listen to a random sound from the dataset\ndef listen_to_random_sound(data,sr):\n  \n    IPython.display.display(IPython.display.Audio(data, rate=sr)) # display is used to display the audio file","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd ..","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_folds = \"working/UrbanSound8K/audio\"\noutput_path=\"./working\"\n\nif \"spectrograms\" not in os.listdir(output_path):\n\n    os.mkdir(f\"{output_path}/spectrograms\")\nnumber_of_files = df.shape[0] # get the rows\nnumber_of_processed = 0\nnumber_of_errors = 0\n\n\nwith open(f\"{output_path}/errors.txt\", \"w\") as error_file:\n    for index, row in df.iterrows(): # iterrows is used to iterate over rows of a dataframe,retrieving the index and the row\n        try:\n            file_name = row[\"slice_file_name\"]\n            fold = row[\"fold\"]\n            classid = row[\"classID\"]\n            path_to_file = f\"{path_to_folds}/fold{fold}/{file_name}\"\n                        \n            data, sr = librosa.load(path_to_file)# load is used to load audio files with librosa,retrieving the time series and the sampling rate\n            # type(data) is numpy.ndarray\n            # type(sr) is int\n            #shape of data is (n,) where n is the number of samples\n            # sr is the sampling rate of the audio file\n            #print(f\" \\r  data type is {type(data)} and sampling rate is {type(sr)} - shape of data is {data.shape} and sampling rate is {sr}\",end=\"\")\n            #listen_to_random_sound(data,sr)\n            \n            spectrogram = create_spectrogram(data)\n            spects_path=f\"{output_path}/spectrograms\"\n            save_spectrogram(spects_path,spectrogram, file_name, classid)\n            \n            del data\n            del sr\n            del spectrogram\n            \n        except Exception as e:\n            number_of_errors += 1\n            error_file.write(f\"{number_of_errors}: {e}\\n\")\n        \n        finally:\n            number_of_processed += 1\n        \n        print(f\"\\rNumber: {number_of_processed}/{number_of_files} | Errors: {number_of_errors}\", end=\"\")\n        \n        #break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and  Save İmage datasets ","metadata":{"id":"1s7GwkpJORUT"}},{"cell_type":"code","source":"# create numpy arrays for the training images and labels\ndef create_training_data(datasetfolder_path,resize=None,grayscale=False,normalize=False):\n\n    training_data = []\n    number_of_errors = 0\n    number_of_processed = 0\n\n    for classid in os.listdir(datasetfolder_path):\n        path_to_class = f\"{datasetfolder_path}/{classid}\"\n        \n        for img in os.listdir(path_to_class):\n            try:\n                if grayscale:\n                    img_array = cv.imread(os.path.join(\n                        path_to_class, img), cv.IMREAD_GRAYSCALE)\n                # imread is used to read an image from the specified file,IMREAD_GRAYSCALE is used to load an image in grayscale mode,retrieving the image as a numpy array\n\n                else:\n                    img_array = cv.imread(f\"{path_to_class}/{img}\")\n\n                if resize is not None:\n                    img_array = cv.resize(img_array, resize)\n                    # resize is used to resize an image,retrieving the resized image as a numpy array\n                \n                if normalize:\n                    img_array = img_array/255.0\n                    # normalize the image array\n\n                training_data=np.append([img_array, classid])\n                number_of_processed += 1\n\n            except:\n                number_of_errors += 1\n\n        print(f\"\\rProcessed: {number_of_processed} | Erros: {number_of_errors} | training data shape : {np.ndarray(training_data).shape} \", end=\"\")\n    return training_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_datasets=create_training_data(datasetfolder_path=f\"{output_path}/spectrograms\")\nlen(img_datasets)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image ,id in images:\n  print(f\"\\r class id :{id}  image :{image.shape}\", end=\"\")","metadata":{"id":"brTVJimyTL7o","outputId":"017f521d-d8a7-4244-b98f-d29ba1adfeb4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# write the csv file for the training data\ndef write_csv(training_data, csv_name):\n    df = pd.DataFrame(training_data, columns=[\"image\", \"classid\"])\n    df.to_csv(csv_name, index=False)\n    print(\"CSV file created\")\n\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_pickle(path,data):\n    if \"pickle_img_dataset\" not in os.listdir(path):\n    os.mkdir(f\"{path}/pickle_img_dataset\")\n\n    with open((f\"{path}/pickle_img_dataset/allimg8732.pickle\", \"wb\") as f:\n      pickle.dump(data, f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"id":"NtFJlQafLLb4","outputId":"200c7211-2226-4c1b-8e9c-39458e2476e0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dir=f\"{output_path}/spectrograms/\"\n","metadata":{"id":"LMzna52zK_ED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = pathlib.Path(dataset_dir)\nimage_count = len(list(data_dir.glob('*/*.png')))\nprint(image_count)","metadata":{"id":"Nga_Hoob94AG","outputId":"c8c3f662-f32b-49f5-a069-aff2439a6e5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spec = list(data_dir.glob('1/*'))\nPIL.Image.open(str(spec[6]))","metadata":{"id":"qMVc71-y-pcr","outputId":"6d882ded-7e5a-4866-a80f-9e6b929b021a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for root, dirs, files in os.walk(dataset_dir, topdown=True):\n    print(\"=\"*50)\n    print(root)\n    print(f\"root type :{type(files)}\")\n    print(f\"root lenth :{len(files)}\")\n    print(files)\n    print(f\"files type :{type(files)}\")\n    print(f\"files lenth :{len(files)}\")\n    print(dirs)\n    print(f\"DİR type :{type(dirs)}\")\n    print(f\"DİR lenth :{len(dirs)}\")\n    print(\"=\"*50)\n    break\n    ","metadata":{"id":"MqBw9qi-xAHi","outputId":"1f73a0c0-4875-4108-e12a-c994480efcf0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for root,dirs, files in os.walk(dataset_dir):\n    for file in (files):\n        print(os.path.join(root, file))\n        break\n    ","metadata":{"id":"P9lYJtNq3GbZ","outputId":"b0ac59d4-f8f1-4225-bb45-67bad2480eb0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{"id":"YK9jk6b8f5pZ"}},{"cell_type":"code","source":"\nclass_name=\"1\" # set to class name\n\ntrain_files = glob.glob(str(os.path.join(dataset_dir,str(class_name)))+'/*')\n\n\nprint(*train_files)\nprint(f\"1 spectogram img lenth :{len(train_files)}\")\ntrain_imgs = [img_to_array(load_img(img)) for img in train_files]\ntrain_imgs = np.array(train_imgs)\ntrain_labels = np.array([fn.split('/')[5].split(\".\")[0].strip() for fn in train_files]) # target , y label\n\nprint('Train dataset shape:', train_imgs.shape)\nprint('Train labels shape:', train_labels.shape)","metadata":{"id":"vW2Y0qRp1Jny","outputId":"50e8d551-2bb2-44b5-c723-1727bfbbf342"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size=(374,500)\nbatch_size=32 # paketler\n\n#Setting train/test split\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \n    directory=dataset_dir,\n    labels=\"inferred\",\n    validation_split=0.3,\n    subset=\"training\",\n    seed=42,\n    image_size=image_size,\n    batch_size=batch_size,\n)\n\nvalid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    directory=dataset_dir,\n    labels=\"inferred\",\n    validation_split=0.3,\n    subset=\"validation\",\n    seed=1007,\n    image_size=image_size,\n    batch_size=batch_size,\n)\n\n","metadata":{"id":"WEdnyOxsK-GH","outputId":"5a964b08-5cc4-424d-9dd0-844679b14e37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"{train_ds.class_names}\")","metadata":{"id":"BqAPH1xC_Cge","outputId":"9dd9d8f8-9576-433a-d8b3-88d19a3b2675"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print how many images are in the training and validation datasets\nprint(\"Number of training images: \", len(train_ds))\nprint(\"Number of validation images: \", len(valid_ds))","metadata":{"id":"sW1f6TE9N0bA","outputId":"81849183-23ec-4ba8-96b3-5b3e9aa30e50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split valid dataset to test and valid\ntest_ds = valid_ds.take(30) # get 100 data from valid dataset\nvalid_ds = valid_ds.skip(30) # skip first 100","metadata":{"id":"VR1uhAoTN5wU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# how many bachs are in the training and validation datasets\nprint(\"Number of training batches: \", len(train_ds))\nprint(\"Number of validation batches: \", len(valid_ds))\nprint(\"Number of test  batches: \", len(test_ds))","metadata":{"id":"gx_5Hyu2N701","outputId":"f17656b1-7f61-4d20-873b-84b485389dd9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_type=[train_ds,valid_ds,test_ds]","metadata":{"id":"JaS0ArE1TsD1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ds in dataset_type:\n  print(type(ds))","metadata":{"id":"9gowy3cs-IvL","outputId":"d7236cb0-0b23-4c0e-ed95-bb0464136e0b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking images and labels shapes (amount of images, height, width, color channels)\nfor image_batch, labels_batch in test_ds.take(2):\n  print(image_batch.shape)\n  print(labels_batch.shape)\n  break","metadata":{"id":"xw2kIT18A0zD","outputId":"80940130-5738-4cf1-c69e-0d0201ea6141"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking images and labels shapes (amount of images, height, width, color channels)\nfor image_batch, labels_batch in valid_ds:\n  print(image_batch.shape)\n  print(labels_batch.shape)\n  break","metadata":{"id":"GncMbIISq3Nh","outputId":"bef34db1-7b02-48b9-d660-00b86786606b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualizaiton","metadata":{"id":"CcCy6z4c-kqL"}},{"cell_type":"code","source":"#Displaying image samples \nplt.figure(figsize=(18, 18))\nfor images, labels in train_ds.take(1):\n    for i in range(batch_size):\n        ax = plt.subplot(4,8,i+1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")","metadata":{"id":"U3kPRAYpB1kO","outputId":"024738b0-6ace-4cd4-e941-462f03b7e507"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Displaying image samples \nplt.figure(figsize=(18, 18))\nfor images, labels in valid_ds.take(1):\n    for i in range(batch_size):\n        ax = plt.subplot(4,8,i+1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")","metadata":{"id":"XeIKzAz8q9Xx","outputId":"58378504-ceb0-4c4c-b01e-d543d0703f2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Displaying image samples \nplt.figure(figsize=(18, 18))\nfor images, labels in test_ds.take(1):\n    for i in range(batch_size):\n        ax = plt.subplot(4,8,i+1)\n        #print(f\"type image {images[i]}\")\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")","metadata":{"id":"JaRjjyD2q9Cc","outputId":"d41abb17-0d80-4220-aeda-8cb785e6505f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{"id":"hcQc1f2BiLot"}},{"cell_type":"markdown","source":"Before we can build our model and start training, we need to apply one simple augmentation the dataset and that is rescaling. We rescale an input in the (0, 255) range to be in the (0,1) range.","metadata":{"id":"6QJ0dI2piLou"}},{"cell_type":"code","source":"\n\n# convert rgb img to gray scale img\ndef convert_to_gray_scale(img):\n   \n    img = tf.image.rgb_to_grayscale(img)\n   \n    print(f\"\\r convert gray :  image shape : {img.shape} | img type: {type(img)} \", end=\"\")\n    return img\n\n","metadata":{"id":"fS8aFSZ0y47t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"dj_Tu43p_64T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to prepare our datasets for modelling\ndef prepare(ds, augment=False, batch_size=None, resized_size=None,grayscale=True):\n    # Define normalisation function\n    normalization_layer = tf.keras.layers.Rescaling(1./255)\n    # gray_scale transfpormayion \n    resize=tf.keras.Sequential(\n        [tf.keras.layers.experimental.preprocessing.Resizing(\n            resized_size[0],\n            resized_size[1],\n            crop_to_aspect_ratio=True,\n        )]\n    )\n     #define data augmentation function\n    flip_and_rotate = tf.keras.Sequential([\n        tf.keras.layers.experimental.preprocessing.RandomFlip(\n            \"horizontal_and_vertical\"),\n        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n    ])\n\n    if grayscale:\n        ds = ds.map(lambda x, y: (convert_to_gray_scale(x), y))\n    if resized_size:\n        ds = ds.map(lambda x, y: (resize(x, training=True), y))\n\n  \n    ds = ds.map(lambda x, y: (normalization_layer(x), y))\n    image_batch, labels_batch = next(iter(ds))\n    print(f\"batch size :{image_batch.shape}\")\n    print(f\"label size :{labels_batch.shape}\")\n    first_image = image_batch[0]\n    # Notice the pixel values are now in `[0,1]`.\n    print(np.min(first_image), np.max(first_image))\n\n    if augment:\n        ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n\n        \n    \n    \n    return ds\n\nimage_size= (187,250)\ntrain_dataset = prepare(train_ds, augment=False, batch_size=batch_size, resized_size=image_size)\nvalid_dataset = prepare(test_ds, augment=False, batch_size=batch_size, resized_size=image_size)\ntest_dataset = prepare(valid_ds, augment=False, batch_size=batch_size, resized_size=image_size)\n\n","metadata":{"id":"zUTJeOtZiLou","outputId":"f63e3b69-1c21-493e-f72e-4dca6cde034d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image_batch, labels_batch in valid_dataset:\n  print(image_batch.shape)\n  print(labels_batch.shape)\n  break","metadata":{"id":"fymkVSDq3Med","outputId":"7d08e03b-6cc9-4b69-d8dc-8cfbe0168d3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Displaying image samples \nplt.figure(figsize=(18, 12))\nfor images, labels in train_dataset.take(1):\n    for i in range(8):\n        ax = plt.subplot(2,4,i+1)\n        #print(f\"type image {images[i]}\")\n        plt.imshow(images[i].numpy().squeeze(axis=2),cmap=\"gray\")\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")","metadata":{"id":"6n2_EUpq0vvM","outputId":"99f5fcdf-7935-423a-981b-4234f0af484d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{"id":"iQKd5RauPB82"}},{"cell_type":"code","source":"#Checking if the data format i.e the RGB channel is coming first or last so, whatever it may be, model will check first and then input shape will be feeded accordingly.\nfrom keras import backend as K\nimage_size= (187,250)\nimg_height=image_size[0]\nimg_width=image_size[1]\nif K.image_data_format() == \"channels_first\":\n    input_shape = (1, img_height, img_width)\nelse:\n    input_shape = (img_height, img_width, 1)\n","metadata":{"id":"o5EdaBRDPGd6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"id":"1GzWjmyCKF5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv_net(select_model=1): \n  model = tf.keras.Sequential()\n  if select_model==1:\n\n    model.add(tf.keras.layers.Conv2D(96, kernel_size = (3,3), input_shape = input_shape, strides = (2,2), activation = 'relu'))\n\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size = (3,3),strides = (2,2))) \n    model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1) ,activation='relu', padding=\"same\")) \n    model.add(tf.keras.layers.BatchNormalization()) \n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2))) \n    model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")) \n    model.add(tf.keras.layers.BatchNormalization()) \n    model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")) \n    model.add(tf.keras.layers.BatchNormalization()) \n    model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")) \n    model.add(tf.keras.layers.BatchNormalization()) \n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2))) \n\n    model.add(tf.keras.layers.Flatten()) \n\n    model.add(tf.keras.layers.Dense(4096, activation='relu')) \n    model.add(tf.keras.layers.Dropout(0.5)) \n    model.add(tf.keras.layers.Dense(4096, activation='relu')) \n    model.add(tf.keras.layers.Dropout(0.5)) \n    model.add(tf.keras.layers.Dense(2048, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(1024, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(512, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(256, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(128, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n\n\n    model.add(tf.keras.layers.Dense(10, activation='softmax')) \n\n  elif select_model==2:\n    # Create CNN model\n  \n    model.add(tf.keras.layers.Input(shape=input_shape))\n\n    model.add(tf.keras.layers.Conv2D(\n        32, 3, strides=2, padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.BatchNormalization())\n\n    model.add(tf.keras.layers.Flatten())\n\n    model.add(tf.keras.layers.Dense(256, activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.5))\n\n    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n  elif select_model==3:\n    model.add(tf.keras.layers.Input(shape=input_shape))\n    model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(256, activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n\n  else:\n\n    model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(input_shape)))\n\n    model.add(tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"))\n    model.add(tf.keras.layers.MaxPooling2D())\n    model.add(tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n    model.add(tf.keras.layers.MaxPooling2D())\n    model.add(tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n    model.add(tf.keras.layers.MaxPooling2D())\n    model.add(tf.keras.layers.Dropout(0.5))\n  \n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n\n    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n    \n  return model","metadata":{"id":"4sDxeRqwwDxI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dl=conv_net(select_model=3)","metadata":{"id":"rLQJ4XEJLkj5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Visualization","metadata":{"id":"dkZBKOuciLov"}},{"cell_type":"code","source":"model_dl.summary()\n","metadata":{"id":"7I3CppJTWrSf","outputId":"16ea77ca-3a02-487c-e1d5-14c9e69bcae2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Compile model\nmodel_dl.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer=tf.keras.optimizers.RMSprop(),\n    metrics=['accuracy'],\n)\n\n# Train model for 10 epochs, capture the history\nhistory = model_dl.fit(train_dataset,\n          batch_size=batch_size,\n          verbose=1,\n          epochs=50,\n          validation_data=valid_dataset)","metadata":{"id":"b-vFRrUpRsEn","outputId":"e1536d62-653e-4c91-98a9-b63e135ce983"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ","metadata":{"id":"F1g3cwBwXv13"}},{"cell_type":"code","source":"# model_dl.fit(x_train, y_train,\n#           batch_size=batch_size,\n#           epochs=epochs,\n#           verbose=1,\n#           validation_data=(x_test, y_test))","metadata":{"id":"KQ2cRjQsXucS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# score = model_dl.evaluate(x_test, y_test, verbose=0)\n# score = model_dl.evaluate(test_dataset, verbose=0)\n# print(f\"Loss: {score[0]} - Accuracy: {score[1]}\")\n","metadata":{"id":"JGPF05mLakxN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Saving","metadata":{"id":"zs1dmePNbNV2"}},{"cell_type":"code","source":"model_test = model_dl.save('save_models/spectogram_model.h5')","metadata":{"id":"G9DrBMvzcvIF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{"id":"h3qUBCIbgoWn"}},{"cell_type":"markdown","source":"We can compute the final loss and accuracy score on our valid dataset using the evaluate() function.","metadata":{"id":"zroE6ow-iLow"}},{"cell_type":"code","source":"# Compute the final loss and accuracy\nfinal_loss, final_acc = model_dl.evaluate(valid_dataset, verbose=0)\nprint(\"Final loss: {0:.6f}, final accuracy: {1:.6f}\".format(\n    final_loss, final_acc))\n","metadata":{"id":"Afhq5RAQiLow","outputId":"448baa5b-8c1e-4675-fb3c-538d79f0392f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the loss curves for training and validation.\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values)+1)\n\nplt.figure(figsize=(18, 12))\nplt.plot(epochs, loss_values, 'y', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"id":"60chUuK_gq3y","outputId":"f3ef6302-7eb2-4871-ffe5-bd5f43dcc20e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the accuracy curves for training and validation.\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nepochs = range(1, len(acc_values)+1)\n\nplt.figure(figsize=(18, 12))\nplt.plot(epochs, acc_values, 'y', label='Training accuracy')\nplt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n","metadata":{"id":"mQbvQiiJiLow","outputId":"43a5b608-4b9b-4465-d9f8-adb8aa05d07a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"next(iter(test_dataset))[1].numpy()[8]","metadata":{"id":"L6lSxyQrrqJF","outputId":"b523b530-050c-444a-9354-4c14ef25d84a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testimg=next(iter(test_dataset))[0].numpy()[8]","metadata":{"id":"QRgduNwFsZ1O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testimg.shape","metadata":{"id":"BUpcfeUms7tT","outputId":"090f1e98-5116-4077-e067-c74e9ec22832"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testimg=np.expand_dims(testimg,axis=0)","metadata":{"id":"GHVsal8utDtg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=model_dl.predict(testimg)","metadata":{"id":"bwzeb-bMiLow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"id":"obQn93-qsmGl","outputId":"abfb605c-0862-4d09-ea58-a87260774f1f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argmax(pred)","metadata":{"id":"EQ7Z6elitTEL","outputId":"977cf164-d51b-446a-cfac-1a010f84381c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"dKrnFPCTtbm9"},"execution_count":null,"outputs":[]}]}